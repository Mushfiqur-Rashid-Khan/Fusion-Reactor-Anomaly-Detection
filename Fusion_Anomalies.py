# -*- coding: utf-8 -*-
"""Fusion (Baseline Models).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rJRsPzwaPZNbVzOBjCfV9GCA8CpFgUVy
"""

pip install optuna

import optuna
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.cluster import KMeans

# Load dataset
file_path = '/content/Dataset_Fusion.csv'  # Replace with your CSV file path
df = pd.read_csv(file_path)
df.drop(columns=['Magnetic Field Configuration', 'Target Composition'], inplace=True)

# Apply KMeans++ clustering
kmeans = KMeans(n_clusters=7, init='k-means++', random_state=42)
df['cluster'] = kmeans.fit_predict(df.drop('Ignition', axis=1))  # Replace 'Ignition' with actual target column

# Define features and target
X = df.drop(['cluster'], axis=1)
y = df['cluster']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define Optuna objective function
def objective(trial):
    # Optimize hyperparameters
    rf_n_estimators = trial.suggest_int("rf_n_estimators", 50, 200)
    rf_max_depth = trial.suggest_int("rf_max_depth", 3, 20)

    ada_n_estimators = trial.suggest_int("ada_n_estimators", 50, 200)
    ada_learning_rate = trial.suggest_float("ada_learning_rate", 0.01, 1.0)

    bagging_n_estimators = trial.suggest_int("bagging_n_estimators", 10, 100)
    bagging_max_samples = trial.suggest_float("bagging_max_samples", 0.5, 1.0)

    # Create models with optimized parameters
    rf = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=rf_max_depth, random_state=42)
    adaboost_rf = AdaBoostClassifier(n_estimators=ada_n_estimators, learning_rate=ada_learning_rate, random_state=42)
    bagging = BaggingClassifier(n_estimators=bagging_n_estimators, max_samples=bagging_max_samples, random_state=42)

    # Voting ensembles
    voting_bagging = VotingClassifier(estimators=[
        ('bagging', bagging),
        ('rf', rf)
    ], voting='soft')

    voting_adaboost = VotingClassifier(estimators=[
        ('adaboost', adaboost_rf),
        ('rf', rf)
    ], voting='soft')

    rf_bagging = BaggingClassifier(n_estimators=bagging_n_estimators, random_state=42)

    # Final Hybrid Model
    hybrid_model = VotingClassifier(estimators=[
        ('adaboost_rf', adaboost_rf),
        ('voting_bagging', voting_bagging),
        ('voting_adaboost', voting_adaboost),
        ('rf_bagging', rf_bagging)
    ], voting='soft')

    # Train & Evaluate
    hybrid_model.fit(X_train, y_train)
    y_pred = hybrid_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

# Run Optuna
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)

# Print Best Parameters
print("Best Parameters:", study.best_params)

# Train final model with best parameters
best_params = study.best_params

rf_final = RandomForestClassifier(n_estimators=best_params["rf_n_estimators"], max_depth=best_params["rf_max_depth"], random_state=42)
adaboost_final = AdaBoostClassifier(n_estimators=best_params["ada_n_estimators"], learning_rate=best_params["ada_learning_rate"], random_state=42)
bagging_final = BaggingClassifier(n_estimators=best_params["bagging_n_estimators"], max_samples=best_params["bagging_max_samples"], random_state=42)

# Voting ensembles with best parameters
voting_bagging_final = VotingClassifier(estimators=[
    ('bagging', bagging_final),
    ('rf', rf_final)
], voting='soft')

voting_adaboost_final = VotingClassifier(estimators=[
    ('adaboost', adaboost_final),
    ('rf', rf_final)
], voting='soft')

rf_bagging_final = BaggingClassifier(n_estimators=best_params["bagging_n_estimators"], random_state=42)

# Final Optimized Hybrid Model
optimized_hybrid_model = VotingClassifier(estimators=[
    ('adaboost_rf', adaboost_final),
    ('voting_bagging', voting_bagging_final),
    ('voting_adaboost', voting_adaboost_final),
    ('rf_bagging', rf_bagging_final)
], voting='soft')

# Train and Evaluate the Optimized Hybrid Model
optimized_hybrid_model.fit(X_train, y_train)
y_pred_final = optimized_hybrid_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred_final)

print(f"Optimized Hybrid Model Accuracy: {final_accuracy * 100:.5f}%")

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_final)

# Accuracy
accuracy = accuracy_score(y_test, y_pred_final)
print(f"Accuracy: {accuracy * 100:.4f}%")

# Precision, Recall, F1-Score (Weighted Average)
precision = precision_score(y_test, y_pred_final, average='weighted')
recall = recall_score(y_test, y_pred_final, average='weighted')
f1 = f1_score(y_test, y_pred_final, average='weighted')

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Specificity: True Negative / (True Negative + False Positive)
specificity = cm[1, 1] / (cm[1, 1] + cm[0, 1])
print(f"Specificity: {specificity:.4f}")

# Plotting the Confusion Matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'PD', 'RE', 'SMQ', 'TRC', 'CSF', 'DFWE'],
            yticklabels=['Normal', 'PD', 'RE', 'SMQ', 'TRC', 'CSF', 'DFWE'])

plt.title('Classification Result of Fusion Reactor Anomalies with Hybrid OptRAAL')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

import time
# Measure inference time
start_time = time.time()
predictions = optimized_hybrid_model.predict(X_test)
end_time = time.time()

inference_time = (end_time - start_time) / len(X_test)  # Average inference time per sample
print(f"Average inference time per sample: {inference_time:.6f} seconds")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.ensemble import ExtraTreesClassifier
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans

# Load dataset
file_path = '/content/Dataset_Fusion.csv'  # Replace with your CSV file path
df = pd.read_csv(file_path)
df.drop(columns=['Magnetic Field Configuration', 'Target Composition'], inplace=True)

# Apply KMeans++ clustering
kmeans = KMeans(n_clusters=7, init='k-means++', random_state=42)
df['cluster'] = kmeans.fit_predict(df.drop('Ignition', axis=1))  # Replace 'Ignition' with actual target column

# Define features and target
X = df.drop(['cluster'], axis=1)
y = df['cluster']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize models
models = {
    "XGBoost": XGBClassifier(),
    "LightGBM": lgb.LGBMClassifier(),
    "SVM": SVC(),
    "GBDT++": GradientBoostingClassifier(),
    "Extra Trees": ExtraTreesClassifier()
}

# Train models and evaluate accuracy
for model_name, model in models.items():
    # Fit the model
    model.fit(X_train, y_train)

    # Predict on test data
    y_pred = model.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Print the result
    print(f"{model_name} Accuracy: {accuracy:.4f}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Add KNN and Decision Tree to the model dictionary
models.update({
    "KNN": KNeighborsClassifier(n_neighbors=100),  # Using k=5 as default
    "Decision Tree": DecisionTreeClassifier()
})

# Train and evaluate all models
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} Accuracy: {accuracy:.4f}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

# Add KNN and Gaussian Naive Bayes to the models
models = {
    "XGBoost": XGBClassifier(),
    "LightGBM": lgb.LGBMClassifier(),
    "SVM": SVC(),
    "Gaussian Naive Bayes": GaussianNB()
}

# Train models and evaluate accuracy
for model_name, model in models.items():
    # Fit the model
    model.fit(X_train, y_train)

    # Predict on test data
    y_pred = model.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Print the result
    print(f"{model_name} Accuracy: {accuracy:.4f}")